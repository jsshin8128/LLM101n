{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d8158dfddb2ac72",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "\n",
    "In this assignment, you will continue with the Bigram Language Model from the Lecture. Make the training loop and inference for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c011cbf15834af26",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11b5f1da-02d0-4a59-87c8-87f5a169d87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/user/LLM101n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from utils import load_text, set_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4a0b5274f4dca2",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef1253f6800c36c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BigramConfig:\n",
    "    root_dir: str = os.getcwd() + \"/../../\"\n",
    "    dataset_path: str = \"data/names.txt\"\n",
    "\n",
    "    # Tokenizer\n",
    "    vocab_size: int = 0  # Set later\n",
    "\n",
    "    seed: int = 101\n",
    "    \n",
    "config = BigramConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5726a09b7e375389",
   "metadata": {},
   "source": [
    "## Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e4d38445588cdbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to 101\n"
     ]
    }
   ],
   "source": [
    "set_seed(config.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74be7e01434a14b",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32e86ea6f15f11b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded text data from /home/user/LLM101n/notebooks/Assignments/../../data/names.txt (length: 228145 characters).\n"
     ]
    }
   ],
   "source": [
    "names = load_text(config.root_dir + config.dataset_path).splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aff6f7e1bbade4",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed24c0db0ce9da98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add special token\n",
    "names = [\".\" + name + \".\" for name in names]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e38655c11342dd",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a2f768e619dfb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = [chr(i) for i in range(97, 123)]  # all alphabet characters\n",
    "chars.insert(0, \".\")  # Add special token\n",
    "config.vocab_size = len(chars)\n",
    "str2idx = {char: idx for idx, char in enumerate(chars)}\n",
    "idx2str = {idx: char for char, idx in str2idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e4e1dda633584d",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "523dd8edb6b3e9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights\n",
    "W = torch.randn(config.vocab_size, config.vocab_size, requires_grad=True)\n",
    "b = torch.randn(config.vocab_size, requires_grad=True)\n",
    "params = [W, b]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ca7979aafb68a",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6125c93c9c7519",
   "metadata": {},
   "source": [
    "#### Task 1: Train Bigram Language Model (Neural Network Approach)\n",
    "\n",
    "Make the training loop for the Bigram Language Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8f5165e72e37f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set of Input, Target pairs\n",
    "inputs, targets = [], []\n",
    "for name in names:\n",
    "    for char1, char2 in zip(name, name[1:]):\n",
    "        input = str2idx[char1]\n",
    "        target = str2idx[char2]\n",
    "        inputs.append(input)\n",
    "        targets.append(target)\n",
    "\n",
    "# Convert to tensor\n",
    "inputs = torch.tensor(inputs, dtype=torch.long)\n",
    "targets = torch.tensor(targets, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebe57ba03f098d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Input, Target pairs: 228146\n",
      "Input shape: torch.Size([228146])\n",
      "Target shape: torch.Size([228146])\n",
      "First (Input, Target): (0, 5)\n",
      "Second (Input, Target): (5, 13)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of Input, Target pairs: {len(inputs)}\")\n",
    "print(f\"Input shape: {inputs.shape}\")\n",
    "print(f\"Target shape: {targets.shape}\")\n",
    "print(f\"First (Input, Target): ({inputs[0]}, {targets[0]})\")\n",
    "print(f\"Second (Input, Target): ({inputs[1]}, {targets[1]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4baab50fd5515e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# One-hot encode the input tensor.                                             #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "inputs_encoded = F.one_hot(inputs, num_classes=config.vocab_size)\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "# Convert data type to float\n",
    "inputs_encoded = inputs_encoded.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a5ef07b0b820e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10, Loss 2.8777\n",
      "Step 20, Loss 2.7156\n",
      "Step 30, Loss 2.6413\n",
      "Step 40, Loss 2.5989\n",
      "Step 50, Loss 2.5718\n",
      "Step 60, Loss 2.5532\n",
      "Step 70, Loss 2.5397\n",
      "Step 80, Loss 2.5294\n",
      "Step 90, Loss 2.5214\n",
      "Step 100, Loss 2.5150\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "steps = 100\n",
    "lr = 10\n",
    "\n",
    "for step in range(1, steps + 1):\n",
    "    # Forward pass\n",
    "    ################################################################################\n",
    "    # TODO:                                                                        #\n",
    "    # Implement the forward pass.                                                  #\n",
    "    ################################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    logits = inputs_encoded @ W + b\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    \n",
    "    # loss\n",
    "    log_probs = torch.log(probs + 1e-9)  # Add small value to prevent log(0)\n",
    "    loss = -log_probs[torch.arange(len(targets)), targets].mean()\n",
    "    \n",
    "    # Backward pass\n",
    "    ################################################################################\n",
    "    # TODO:                                                                        #\n",
    "    # Implement the backward pass.                                                 #\n",
    "    ################################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    for param in params:\n",
    "        if param.grad is not None:\n",
    "            param.grad.zero_()\n",
    "    loss.backward()\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    \n",
    "    # Update weights\n",
    "    ################################################################################\n",
    "    # TODO:                                                                        #\n",
    "    # Update the weights of the model using the gradients.                         #\n",
    "    ################################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    with torch.no_grad():\n",
    "        for param in params:\n",
    "            param -= lr * param.grad\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    \n",
    "    if step % 10 == 0:\n",
    "        print(f\"Step {step}, Loss {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d956fef0a027963",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786f852b486b92cb",
   "metadata": {},
   "source": [
    "#### Task 2: Generate a Name\n",
    "\n",
    "Create a function to generate a name using the trained Bigram Language Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b31dfacd08b51cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greeema\n",
      "malymaoshyif\n",
      "saion\n",
      "aryn\n",
      "amamerannhalielysuaa\n"
     ]
    }
   ],
   "source": [
    "# Create a function to generate a name\n",
    "def generate_name():\n",
    "    new_name = []\n",
    "    start_idx = str2idx[\".\"]\n",
    "    \n",
    "    while True:\n",
    "        ################################################################################\n",
    "        # TODO:                                                                        #\n",
    "        # 1. Forward pass                                                              #\n",
    "        # 2. Sample the next token                                                     #\n",
    "        # 3. Decode the token                                                          #\n",
    "        # 4. Update the start_idx                                                      #\n",
    "        # 5. Break if the next character is \".\"                                        #\n",
    "        ################################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        # Forward pass\n",
    "        x = F.one_hot(torch.tensor([start_idx]), num_classes=config.vocab_size).float()\n",
    "        logits = x @ W + b\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        # Sample\n",
    "        next_idx = torch.multinomial(probs, num_samples=1).item()\n",
    "        # Decode\n",
    "        next_char = idx2str[next_idx]\n",
    "        # Break if \".\"\n",
    "        if next_char == \".\":\n",
    "            break\n",
    "        new_name.append(next_char)\n",
    "        # Update\n",
    "        start_idx = next_idx\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    return ''.join(new_name)\n",
    "\n",
    "# Generate 5 names\n",
    "for _ in range(5):\n",
    "    print(generate_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d00faaddbe4b44",
   "metadata": {},
   "source": [
    "## Extra Credit\n",
    "\n",
    "We have already made our own custom auto-grad Tensor class. Let's use it!\n",
    "\n",
    "Train the Bigram Language Model using our custom auto-grad Tensor class.\n",
    "\n",
    "**Do not use any built-in PyTorch functions.** (other deep learning libraries are also prohibited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6f26efc1212bb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "    def __init__(self, data, _children=(), _operation=''):\n",
    "        self.data = data\n",
    "        self._prev = set(_children)\n",
    "        self.gradient = 0\n",
    "        self._backward = lambda: None\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"tensor=({self.data})\"\n",
    "\n",
    "    def __add__(self, other):  # self + other\n",
    "        output = Tensor(self.data + other.data, (self, other), '+')\n",
    "        def _backward():\n",
    "            self.gradient = 1 * output.gradient\n",
    "            other.gradient = 1 * output.gradient\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def __mul__(self, other):  # self * other\n",
    "        output = Tensor(self.data * other.data, (self, other), '*')\n",
    "        def _backward():\n",
    "            self.gradient = other.data * output.gradient\n",
    "            other.gradient = self.data * output.gradient\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def tanh(self):  # tanh(self)\n",
    "        output = Tensor(math.tanh(self.data), (self,), 'tanh')\n",
    "        def _backward():\n",
    "            self.gradient = (1.0 - math.tanh(self.data) ** 2) * output.gradient\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def __pow__(self, power):  # self ** power\n",
    "        assert isinstance(power, (int, float)), \"Power must be an int or a float\"\n",
    "        output = Tensor(self.data ** power, (self,), f'**{power}')\n",
    "        def _backward():\n",
    "            self.gradient = power * (self.data ** (power - 1)) * output.gradient\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "        self.gradient = 1\n",
    "        for node in reversed(topo):\n",
    "            node._backward()\n",
    "\n",
    "    def __neg__(self): # -self\n",
    "        return self * Tensor(-1.0)\n",
    "\n",
    "    def __sub__(self, other): # self - other\n",
    "        return self + (-other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3b7815ada66df8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Loss = 341.2348\n",
      "Step 10, Loss = 341.2348\n",
      "Step 20, Loss = 341.2348\n",
      "Step 30, Loss = 341.2348\n",
      "Step 40, Loss = 341.2348\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "import random\n",
    "import math\n",
    "\n",
    "# 1. 알파벳 문자들을 준비해보자!\n",
    "chars = ['.'] + [chr(i) for i in range(97, 123)]\n",
    "vocab_size = len(chars)\n",
    "\n",
    "str2idx = {ch : i for i, ch in enumerate(chars)}\n",
    "idx2str = {i: ch for ch, i in str2idx.items()}\n",
    "\n",
    "# 2. 이름 데이터 불러오기\n",
    "with open(\"/home/user/LLM101n/data/names.txt\", \"r\") as f:\n",
    "    names = f.read().splitlines()\n",
    "\n",
    "names = [\".\" + name + \".\" for name in names]\n",
    "\n",
    "# 3. (입력글자, 정답글자) pair 만들기\n",
    "inputs = []\n",
    "targets = []\n",
    "`\n",
    "for name in names:\n",
    "    for ch1, ch2 in zip(name, name[1:]):\n",
    "        inputs.append(str2idx[ch1])   \n",
    "        targets.append(str2idx[ch2])\n",
    "\n",
    "inputs = inputs[:100]\n",
    "targets = targets[:100]\n",
    "\n",
    "# 4. 가중치 만들기 \n",
    "W = [[Tensor(random.uniform(-1, 1)) for _ in range(vocab_size)] for _ in range(vocab_size)]\n",
    "b = [Tensor(0.0) for _ in range(vocab_size)]\n",
    "\n",
    "# 5. 확률 계산을 위한 소프트맥스 함수\n",
    "def softmax(logits):\n",
    "    exps = [math.exp(logit.data) for logit in logits]\n",
    "    total = sum(exps)\n",
    "    return [Tensor(exp / total) for exp in exps]\n",
    "\n",
    "# 6. 학습\n",
    "lr = 0.1\n",
    "\n",
    "for step in range(50):  \n",
    "    total_loss = Tensor(0.0)  \n",
    "\n",
    "    for x_idx, y_idx in zip(inputs, targets):\n",
    "        x = [Tensor(1.0 if i == x_idx else 0.0) for i in range(vocab_size)]\n",
    "\n",
    "        logits = []\n",
    "        for j in range(vocab_size):\n",
    "            sum_value = Tensor(0.0)\n",
    "            for i in range(vocab_size):\n",
    "                sum_value = sum_value + x[i] * W[i][j]\n",
    "            logits.append(sum_value + b[j])\n",
    "\n",
    "        probs = softmax(logits)\n",
    "\n",
    "        target_prob = probs[y_idx]\n",
    "        loss = -Tensor(math.log(target_prob.data + 1e-9)) \n",
    "        total_loss = total_loss + loss\n",
    "\n",
    "    total_loss.backward()\n",
    "\n",
    "    for i in range(vocab_size):\n",
    "        for j in range(vocab_size):\n",
    "            W[i][j].data -= lr * W[i][j].gradient\n",
    "            W[i][j].gradient = 0 \n",
    "        b[i].data -= lr * b[i].gradient\n",
    "        b[i].gradient = 0\n",
    "\n",
    "    if step % 10 == 0:\n",
    "        print(f\"Step {step}, Loss = {total_loss.data:.4f}\")\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5d55a8-39c8-416e-952d-415b612ee628",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
