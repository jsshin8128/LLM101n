{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c58786e7d26ad61",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "\n",
    "In this assignment, you will investigate the precision issues in computing the gradient. You will also implement a simple linear regression model using the custom autograd engine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb3e4038597e341",
   "metadata": {},
   "source": [
    "## Task 1: Precision issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d637ecc07f917b6",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.000300000023941"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x):\n",
    "    return 3 * x ** 2 - 4 * x + 5\n",
    "\n",
    "def gradient(f, x, h=0.0001):\n",
    "    return (f(x + h) - f(x)) / h\n",
    "\n",
    "gradient(f, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What happens if we keep decreasing h?\n",
    "gradient(f, 2, h=0.0000000000000001)\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Why is the gradient 0?                                                       #\n",
    "# If you don't know, google it!                                                #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "# ANSWER:\n",
    "# 기울기가 0이 되는 이유는 컴퓨터의 부동소수점 연산 한계 때문.  \n",
    "# h가 너무 작으면 x + h가 x와 같은 값으로 처리되어 f(x + h) - f(x) = 0 이 되어버림.  \n",
    "# 또한, 소수 계산 과정에서 반올림 오류가 발생하여 정확한 기울기를 구하지 못하게 됨.  \n",
    "# 따라서, 너무 작은 h 값은 오히려 계산 오류를 일으킨다!  \n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f38cac6fc0e388d",
   "metadata": {},
   "source": [
    "## Task 2: Linear Regression\n",
    "\n",
    "Let's review the training loop of a simple linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bca901c841e73c5",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3085115ded6fe281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54917eb18f15a26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "    def __init__(self, data, _children=(), _operation=''):\n",
    "        self.data = data\n",
    "        self._prev = set(_children)\n",
    "        self.gradient = 0\n",
    "        self._backward = lambda: None\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"tensor=({self.data})\"\n",
    "\n",
    "    def __add__(self, other):  # self + other\n",
    "        output = Tensor(self.data + other.data, (self, other), '+')\n",
    "        def _backward():\n",
    "            self.gradient = 1 * output.gradient\n",
    "            other.gradient = 1 * output.gradient\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def __mul__(self, other):  # self * other\n",
    "        output = Tensor(self.data * other.data, (self, other), '*')\n",
    "        def _backward():\n",
    "            self.gradient = other.data * output.gradient\n",
    "            other.gradient = self.data * output.gradient\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def tanh(self):  # tanh(self)\n",
    "        output = Tensor(math.tanh(self.data), (self,), 'tanh')\n",
    "        def _backward():\n",
    "            self.gradient = (1.0 - math.tanh(self.data) ** 2) * output.gradient\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def __pow__(self, power):  # self ** power\n",
    "        assert isinstance(power, (int, float)), \"Power must be an int or a float\"\n",
    "        output = Tensor(self.data ** power, (self,), f'**{power}')\n",
    "        def _backward():\n",
    "            self.gradient = power * (self.data ** (power - 1)) * output.gradient\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "        self.gradient = 1\n",
    "        for node in reversed(topo):\n",
    "            node._backward()\n",
    "\n",
    "    def __neg__(self): # -self\n",
    "        return self * Tensor(-1.0)\n",
    "\n",
    "    def __sub__(self, other): # self - other\n",
    "        return self + (-other)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6a0bf8bbd6a560",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "**GOAL: Find the best line that fits the following data.**\n",
    "\n",
    "![Data](../../assets/linear.png)\n",
    "\n",
    "(Image credit: MIT 18.06)\n",
    "\n",
    "(1, 1), (2, 2), (3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76e76d698b2bd862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input, Target data\n",
    "input = [Tensor(1), Tensor(2), Tensor(3)]\n",
    "target = [Tensor(1), Tensor(2), Tensor(2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e07078367a6cf2",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb6d8bd7ee689ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression model\n",
    "class Linear:\n",
    "    def __init__(self):\n",
    "        self.a = Tensor(random.uniform(-1, 1))\n",
    "        self.b = Tensor(random.uniform(-1, 1))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        y = self.a * x + self.b\n",
    "        return y\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.a, self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "115e33323215ad51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor=(-0.4963289027296556)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model = Linear()\n",
    "\n",
    "# Example forward pass\n",
    "print(f\"Output: {model(input[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef292ec8ddb33418",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Implement the training loop for the linear regression model.\n",
    "\n",
    "Choose an appropriate learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a593eadd7f471de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Loss: 9.726734791912419\n",
      "Step: 1, Loss: 9.680729436186892\n",
      "Step: 2, Loss: 9.634934633057055\n",
      "Step: 3, Loss: 9.582688849363606\n",
      "Step: 4, Loss: 9.53080793572553\n",
      "Step: 5, Loss: 9.47928918806143\n",
      "Step: 6, Loss: 9.434419066758046\n",
      "Step: 7, Loss: 9.38356857356327\n",
      "Step: 8, Loss: 9.33307261953566\n",
      "Step: 9, Loss: 9.28292857983918\n"
     ]
    }
   ],
   "source": [
    "lr = 2e-4  # learning rate\n",
    "\n",
    "# Training loop\n",
    "for step in range(10):\n",
    "    total_loss = Tensor(0)\n",
    "    \n",
    "    # Forward pass\n",
    "    for x, y in zip(input, target):\n",
    "        ################################################################################\n",
    "        # TODO:                                                                        #\n",
    "        # Implement the forward pass.                                                  #\n",
    "        ################################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        pred = model(x)\n",
    "        loss = (pred - y) ** 2\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        total_loss += loss\n",
    "\n",
    "    # Backward pass\n",
    "    ################################################################################\n",
    "    # TODO:                                                                        #\n",
    "    # Implement the backward pass.                                                 #\n",
    "    ################################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    total_loss.backward() \n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    # Update weights\n",
    "    ################################################################################\n",
    "    # TODO:                                                                        #\n",
    "    # Update the weights of the model using the gradients.                         #\n",
    "    ################################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    for param in model.parameters():\n",
    "            param.data -= lr * param.gradient\n",
    "            param.gradient = 0\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "            \n",
    "    print(f\"Step: {step}, Loss: {total_loss.data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bf143c6ea52981f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 0.39986504260655226*x + -0.8695745917248353\n"
     ]
    }
   ],
   "source": [
    "# Print the final weights of the model\n",
    "print(f\"y = {model.a.data}*x + {model.b.data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f780baeabe64ada1",
   "metadata": {},
   "source": [
    "## Extra Credit\n",
    "\n",
    "Linear regression is the simplest form of neural networks. It actually does not require gradient descent to solve for the weights.\n",
    "\n",
    "**Find a way to get the weights of the linear regression model without using gradient descent.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "969d8988286f10fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "해석적 해: y = 0.5000 * x + 0.6667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function print(*args, sep=' ', end='\\n', file=None, flush=False)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# y = ax + b                                                                   #\n",
    "# x, y = (1, 1), (2, 2), (3, 2)                                                #\n",
    "# Find the values of a and b without using gradient descent.                   #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([1,2,3])\n",
    "Y = np.array([1,2,2])\n",
    "\n",
    "X_matrix = np.vstack([X, np.ones(len(X))]).T \n",
    "\n",
    "w = np.linalg.inv(X_matrix.T @ X_matrix) @ X_matrix.T @ Y\n",
    "\n",
    "a, b = w\n",
    "\n",
    "print(f\"해석적 해: y = {a:.4f} * x + {b:.4f}\")\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8017f72-7292-43e6-b98a-1ab194f4a55e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
